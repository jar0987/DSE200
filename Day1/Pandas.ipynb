{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNGURAX0Ycjy"
      },
      "source": [
        "## Polars vs Dask\n",
        "\n",
        "- **Load**: CSV, Excel, Parquet, SQLite (via pandas interop where needed)\n",
        "- **Transform**: filter, select, derive columns, groupby/aggregate\n",
        "- **Compute model**: Polars eager vs lazy; Dask lazy with `.compute()`\n",
        "\n",
        "We'll use an included `temperatures.csv` and generate equivalent Excel/Parquet/SQLite files for demos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZYiXKSXYcjz"
      },
      "outputs": [],
      "source": [
        "%pip install polars\n",
        "%pip install dask[dataframe]\n",
        "%pip install pandas\n",
        "%pip install pyarrow\n",
        "%pip install openpyxl\n",
        "%pip install sqlite-utils\n",
        "%pip install fastexcel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQcKNKUgYcjz"
      },
      "outputs": [],
      "source": [
        "# grab the data if not exists\n",
        "\n",
        "import urllib.request\n",
        "import os\n",
        "\n",
        "csv_path = \"temperatures.csv\"\n",
        "excel_path = \"temperatures.xlsx\"\n",
        "parquet_path = \"temperatures.parquet\"\n",
        "sqlite_path = \"temperatures.sqlite\"\n",
        "\n",
        "if not os.path.exists(csv_path):\n",
        "    url = 'https://raw.githubusercontent.com/DSE200/Day1/main/temperatures.csv'\n",
        "    try:\n",
        "        urllib.request.urlretrieve(url, \"temperatures.csv\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to download temperatures.csv from {url}: {e}\")\n",
        "\n",
        "if not os.path.exists(excel_path):\n",
        "    url = 'https://raw.githubusercontent.com/DSE200/Day1/main/temperatures.xlsx'\n",
        "    try:\n",
        "        urllib.request.urlretrieve(url, \"temperatures.xlsx\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to download temperatures.xlsx from {url}: {e}\")\n",
        "\n",
        "if not os.path.exists(parquet_path):\n",
        "    url = 'https://raw.githubusercontent.com/DSE200/Day1/main/temperatures.parquet'\n",
        "    try:\n",
        "        urllib.request.urlretrieve(url, \"temperatures.parquet\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to download temperatures.parquet from {url}: {e}\")\n",
        "\n",
        "if not os.path.exists(sqlite_path):\n",
        "    url = 'https://raw.githubusercontent.com/DSE200/Day1/main/temperatures.sqlite'\n",
        "    try:\n",
        "        urllib.request.urlretrieve(url, \"temperatures.sqlite\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to download temperatures.sqlite from {url}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqGeXTohYcjz"
      },
      "outputs": [],
      "source": [
        "# Loading data with Pandas\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "\n",
        "\n",
        "table_name = \"temperatures\"\n",
        "\n",
        "pd_csv = pd.read_csv(csv_path)\n",
        "pd_parquet = pd.read_parquet(parquet_path)\n",
        "pd_excel = pd.read_excel(excel_path)\n",
        "# SQLite\n",
        "with sqlite3.connect(sqlite_path) as conn:\n",
        "    pd_sql = pd.read_sql(f\"SELECT * FROM {table_name}\", conn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPvD7_ZRYcj0"
      },
      "outputs": [],
      "source": [
        "# Simple transformations with Pandas\n",
        "\n",
        "pd_res = (\n",
        "    pd_csv.dropna(subset=[\"city\", \"temperature (C)\"]) # drop the rows with null city and temperature (c)\n",
        "         .rename(columns={\"temperature (C)\": \"temp_C\"}) # rename the temperature (c) to temp_C\n",
        ")\n",
        "pd_res[\"temp_F\"] = pd_res[\"temp_C\"] * 9/5 + 32 # create a new column temp_F\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnKH4HN4Ycj0"
      },
      "outputs": [],
      "source": [
        "# Group by city and compute mean temp in C/F\n",
        "pd_res = (pd_res.groupby(\"city\")[[\"temp_C\", \"temp_F\"]].mean()\n",
        "                 .rename(columns={\"temp_C\": \"mean_C\", \"temp_F\": \"mean_F\"})\n",
        "                 .reset_index()\n",
        "                 .sort_values(\"mean_C\", ascending=False))\n",
        "pd_res.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e8dorIOYcj0"
      },
      "outputs": [],
      "source": [
        "# Introduction to Polars and dask\n",
        "import polars as pl\n",
        "\n",
        "\n",
        "\n",
        "# Polars: eager reads\n",
        "pl_csv = pl.read_csv(csv_path)\n",
        "pl_parquet = pl.read_parquet(parquet_path)\n",
        "pl_excel = pl.read_excel(excel_path)\n",
        "\n",
        "# Polars: SQLite via pandas interop\n",
        "with sqlite3.connect(sqlite_path) as conn:\n",
        "    pd_sql = pd.read_sql(f\"SELECT * FROM {table_name}\", conn)\n",
        "pl_sql = pl.from_pandas(pd_sql)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApfjnXWHYcj0"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import dask.dataframe as dd\n",
        "\n",
        "# Dask: lazy reads (need .compute())\n",
        "# CSV\n",
        "dd_csv = dd.read_csv(csv_path) # nothing is actually read yet until we call compute\n",
        "\n",
        "# Dask read parquet\n",
        "dd_parquet = dd.read_parquet(parquet_path)\n",
        "\n",
        "# Excel via pandas + from_pandas\n",
        "pd_xl = pd.read_excel(excel_path)\n",
        "dd_excel = dd.from_pandas(pd_xl, npartitions=1)\n",
        "\n",
        "# SQLite via pandas + from_pandas\n",
        "with sqlite3.connect(sqlite_path) as conn:\n",
        "    pd_sql = pd.read_sql(f\"SELECT * FROM {table_name}\", conn)\n",
        "dd_sql = dd.from_pandas(pd_sql, npartitions=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOhByEdpYcj0"
      },
      "source": [
        "### Comparing\n",
        "Goal: select `date`, `city`, keep temp > 25Â°C, top 5 by temp.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAeFpBxbYcj0"
      },
      "outputs": [],
      "source": [
        "# pandas\n",
        "mini_pd = (pd_csv\n",
        "    .rename(columns={\"temperature (C)\": \"temp_C\"})\n",
        "    .loc[lambda d: d[\"temp_C\"] > 25, [\"date\", \"city\", \"temp_C\"]]\n",
        "    .sort_values(\"temp_C\", ascending=False)\n",
        "    .head(5).reset_index(drop=True)\n",
        ")\n",
        "mini_pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEryZ9fcYcj0"
      },
      "outputs": [],
      "source": [
        "# Polars\n",
        "mini_pl = (pl_csv\n",
        "    .with_columns(pl.col(\"temperature (C)\").alias(\"temp_C\"))\n",
        "    .filter(pl.col(\"temp_C\") > 25)\n",
        "    .select([\"date\", \"city\", \"temp_C\"])\n",
        "    .sort(\"temp_C\", descending=True)\n",
        "    .head(5)\n",
        ")\n",
        "mini_pl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhFqLhVgYcj0"
      },
      "outputs": [],
      "source": [
        "# Dask\n",
        "mini_dd = (dd_csv\n",
        "    .rename(columns={\"temperature (C)\": \"temp_C\"})\n",
        "    .loc[lambda d: d[\"temp_C\"] > 25, [\"date\", \"city\", \"temp_C\"]]\n",
        "    .nlargest(5, \"temp_C\").reset_index(drop=True)\n",
        ")\n",
        "mini_dd.compute()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSnWvRX0Ycj0"
      },
      "source": [
        "### Side-by-side: filter, select, derive, groupby\n",
        "We'll demonstrate identical logic:\n",
        "- Keep rows with non-null `city` and `temperature (C)`\n",
        "- Derive `temp_F = temperature (C) * 9/5 + 32`\n",
        "- Group by `city`, compute mean temp in C/F\n",
        "- Sort by mean temp desc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEkyopv2Ycj1"
      },
      "outputs": [],
      "source": [
        "# Polars (eager)\n",
        "res_pl = (\n",
        "    pl_csv\n",
        "    .filter(pl.col(\"city\").is_not_null() & pl.col(\"temperature (C)\").is_not_null())\n",
        "    .with_columns(pl.col(\"temperature (C)\").alias(\"temp_C\"))\n",
        "    .with_columns((pl.col(\"temp_C\") * 9/5 + 32).alias(\"temp_F\"))\n",
        "    .group_by(\"city\")\n",
        "    .agg([pl.col(\"temp_C\").mean().alias(\"mean_C\"), pl.col(\"temp_F\").mean().alias(\"mean_F\")])\n",
        "    .sort(pl.col(\"mean_C\"), descending=True)\n",
        ")\n",
        "res_pl.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Dawu96iYcj1"
      },
      "source": [
        "### Lazy vs Eager at a glance\n",
        "- **Polars eager**: `pl.read_csv(...).filter(...).group_by(...).collect()` is immediate unless using lazy.\n",
        "- **Polars lazy**: use `pl.scan_csv(...)` to build a plan, `collect()` to execute.\n",
        "- **Dask**: always lazy; chain ops, then `.compute()` to execute.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mD-PDntzYcj1"
      },
      "outputs": [],
      "source": [
        "# Polars LAZY pipeline\n",
        "dsl = (\n",
        "    pl.scan_csv(str(csv_path))\n",
        "    .filter(pl.col(\"city\").is_not_null() & pl.col(\"temperature (C)\").is_not_null())\n",
        "    .with_columns(pl.col(\"temperature (C)\").alias(\"temp_C\"))\n",
        "    .with_columns((pl.col(\"temp_C\") * 9/5 + 32).alias(\"temp_F\"))\n",
        "    .group_by(\"city\")\n",
        "    .agg([pl.col(\"temp_C\").mean().alias(\"mean_C\"), pl.col(\"temp_F\").mean().alias(\"mean_F\")])\n",
        "    .sort(pl.col(\"mean_C\"), descending=True)\n",
        ")\n",
        "# Explain physical plan (optional)\n",
        "print(\"Polars lazy plan:\")\n",
        "print(dsl.explain())\n",
        "res_pl_lazy = dsl.collect().head(5)\n",
        "res_pl_lazy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znnDdqZ5Ycj1"
      },
      "source": [
        "1. Grab and scan the CSV file and grab 2 out of 3 columns (Temperature and city)\n",
        "2. Select only the non null rows\n",
        "3. Rename the column temperature c\n",
        "4. Create column for temp_F\n",
        "5. Group By city\n",
        "5. Sort\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wUmGPq5Ycj1"
      },
      "outputs": [],
      "source": [
        "# Dask lazy pipeline (always lazy)\n",
        "pipe = (\n",
        "    dd.read_csv(str(csv_path))\n",
        "    .dropna(subset=[\"city\", \"temperature (C)\"])\n",
        "    .rename(columns={\"temperature (C)\": \"temp_C\"})\n",
        "    .assign(temp_F=lambda x: x[\"temp_C\"] * 9/5 + 32)\n",
        "    .groupby(\"city\")[\"temp_C\", \"temp_F\"].mean().reset_index()\n",
        "    .rename(columns={\"temp_C\": \"mean_C\", \"temp_F\": \"mean_F\"})\n",
        ")\n",
        "print(pipe)\n",
        "res_dask_lazy = pipe.compute().sort_values(\"mean_C\", ascending=False).head(5)\n",
        "res_dask_lazy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6_dVqd3Ycj1"
      },
      "source": [
        "Polars Lazy vs Dask\n",
        "1. Polars operate in RAM vs Dask runs through the partitions to support larger than memory datasets\n",
        "2. Polars is single threaded vs Multi threaded dask\n",
        "3. Polars has advanced query optimization similar to database system. Dask's priority is scallability and out of core datasets."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}